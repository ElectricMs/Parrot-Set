同一个 query 在你的对照脚本里被检索了两次，所以会触发两次“query embedding”。

- 第 1 次：baseline 分支（纯向量检索）调用了 Chroma 的 collection.query(...)，Chroma 会用 embedding_function 对这条 query 做一次 embedding，所以打印一条：[embedding] chunks=1 ...

- 第 2 次：rerank 分支调用 rag.retrieve(...)，它内部也会先做一次向量粗排（同样是 collection.query(...)），所以又做了一次 query embedding，再打印一条。

你终端里看到的两行：

- chunks=1 total_tokens=24 ...

- chunks=1 total_tokens=24 ...

就是“同一个 query 被 baseline 和 rerank 各 embed 一次”。（reranker 本身不走 embedding，这两条都来自向量检索阶段。）

如果你不想看到两次：

- 只跑 rerank：不要做 baseline 对照（或者加个参数开关跳过 baseline）。

- 只跑 baseline：同理跳过 rerank。





差这么多是正常现象：向量检索和 Cross-Encoder rerank 的计算量不是一个量级。

### 为什么向量 Top-10 只要 ~300ms

- 主要成本是：

- 1 次 query embedding（你日志里 chunks=1 total_tokens≈24，输入很短）

- 向量库（Chroma/HNSW）做一次 ANN 检索（基本是索引查找）

- 这类工作属于“一次向量化 + 一次索引查询”，非常省。

### 为什么 rerank 要 ~3000ms（CPU 上尤其明显）

你现在用的是 sentence-transformers 的 CrossEncoder + BAAI/bge-reranker-base + CPU。它的特点是：

- 计算复杂度更高：对每个候选都要跑一次“query+doc 拼接后”的 Transformer 前向

- 你是 Top-10 → 就是 10 个 (query, doc) pairs

- 即使批量推理，依然是 10 条长序列的 transformer 推理，远比 1 次短 query embedding 重。

- 序列更长：rerank 输入是 query + chunk内容

- chunk 虽然是 ~500 字符，但经 tokenizer 后可能接近模型的 max_length 截断上限（常见 512 tokens），每条 pair 都更“重”。

- CPU 推理慢：bge-reranker-base 模型本身不小（你终端里也下载了约 1.11G 的权重），CPU 上每次前向本来就容易到秒级。

### 你可以如何验证“差异来自 rerank 本身”

看控制台/日志里的这一行（我们在 agent/rag.py 里打了）：

- [rerank] pairs=10 time_ms=XXXX

这个 time_ms 基本就对应你看到的 “rerank 需要 ~3000ms”。

### 常见优化方向（按收益从大到小）

- 上 GPU：同模型从秒级降到几十/几百毫秒很常见。

- 换更小/更快的 reranker：或用量化版本（如果你接受）。

- 减小 coarse_k：比如先 Top-10 改 Top-5（精排成本近似线性下降）。

- 限制 rerank 输入长度：对 chunk 做更激进截断（rerank 只看前 N tokens/字符）。

- 调 batch_size / torch 线程数：CPU 上有时能改善吞吐（视机器而定）。









rerank 模型不是“比较两个向量”，而是把“问题 + 候选段落”当成一段输入文本，整段丢进 Transformer 里算一个相关性分数。

### 向量检索在做什么（便宜）

- 先把 query 编成一个向量 q*q*

- 向量库里每个 chunk 已经有向量 di*d**i*

- 相关性就是算 q⋅di*q*⋅*d**i*（或 cosine），这是很便宜的向量运算，再配合索引快速找 Top-10。

### Cross-Encoder rerank 在做什么（贵）

对每个候选 chunk，都要单独算一次：

- 构造输入：[CLS] query [SEP] doc_chunk [SEP]

- Transformer 逐层做注意力计算，最后输出一个分数（比如 logits）

所以 Top-10 rerank 实际上是做 10 次（或批量一次算 10 条）Transformer 前向推理。

### 为什么会贵：序列长度决定成本

Transformer 自注意力的核心计算量和序列长度 L*L* 的关系大致是 \(O(L^2)\)（每个 token 都要和其它 token 交互）。

- embedding 你的 query 很短（几十 tokens）

- rerank 的输入是 “query + chunk”，chunk 常常接近几百 tokens

→ L*L* 变大很多，单次前向就重很多；再乘以候选数量（10 个），总耗时就到秒级（CPU 更明显）。

一句话：向量检索=向量点积 + 索引检索；rerank=对每个候选跑一遍“读懂全文”的大模型推理。



### 为了评估你提到的 3 类成本，需要准备哪些数据

#### 1) Embedding 成本（离线建库 + 在线查询 embedding）

至少要有：

- embedding 模型与单价：model 名称 + 计费单位（常见是“每 1K/1M tokens”或“每 1K 字符”等）

- embedding 调用次数：

- 离线建库：总 chunk 数、每 chunk 生成 embedding 的次数（是否重试/重建）

- 在线查询：每个 query 是否做了 query embedding（通常 1 次/请求；如果做 query 改写、多路召回，可能 >1）

- embedding 输入规模：

- 每次 embedding 的输入 token 数（推荐）或字符数

- 汇总：总 tokens（建库总 tokens、在线总 tokens）、平均 tokens/次

计算口径（示例）：

- 离线建库 embedding 成本 = 建库 embedding 总 tokens × embedding 单价

- 在线每请求 embedding 成本 = 平均 query-embedding tokens × 单价 × 平均 embedding 次数/请求

```
[embedding] chunks=64 total_tokens=14818 time_ms=67425.37
INFO:agent.rag:[embedding] chunks=64 total_tokens=14818 time_ms=67425.37
[embedding] chunks=64 total_tokens=15745 time_ms=79189.25
INFO:agent.rag:[embedding] chunks=64 total_tokens=15745 time_ms=79189.25
[embedding] chunks=64 total_tokens=15156 time_ms=81257.12
INFO:agent.rag:[embedding] chunks=64 total_tokens=15156 time_ms=81257.12
[embedding] chunks=25 total_tokens=5889 time_ms=31190.97
INFO:agent.rag:[embedding] chunks=25 total_tokens=5889 time_ms=31190.97
```





#### 2) LLM token 消耗（RAG 生成/改写/重排等）

至少要有（按每个 LLM 调用记录）：

- model 名称与计价：输入 token 单价、输出 token 单价

- usage：prompt_tokens、completion_tokens、total_tokens

- 调用链路：这次调用属于哪一步（query rewrite / answer generation / rerank-by-LLM / tool calling 等）

- 请求级汇总：每个用户请求触发了几次 LLM（平均/分位数）

计算口径（示例）：

- 单次 LLM 成本 = prompt_tokens × 输入单价 + completion_tokens × 输出单价

- 在线每请求 LLM 成本 = 该请求所有 LLM 调用成本求和

- 进一步可算：P50/P95 成本（更贴近“坏天气”）

#### 3) 每 QPS 成本（在线服务运行时成本）

这里一般分两块：变量成本 + 固定成本：

- 变量成本（随请求数线性）

- 每请求 embedding 成本（上面算出来）

- 每请求 LLM 成本（上面算出来）

- （可选）向量库/数据库按请求计费（如果云服务按读写/查询计费）

- 固定成本（随时间）

- 推理服务/网关/向量库实例的 $/hour

- 机器规格、实例数量、GPU/CPU、存储（embedding 向量体积）、网络出入流量

- 缓存（Redis 等）成本

计算口径（示例）：

- 每秒成本 = QPS ×（每请求变量成本） +（固定成本/秒）

- 每请求总成本 = 每请求变量成本 + 固定成本/秒 ÷ QPS

（因此 QPS 越高，固定成本摊薄越明显）