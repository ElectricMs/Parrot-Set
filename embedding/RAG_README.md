# 简易 RAG 流程演示

基于嵌入检索和 Qwen3-VL 模型的 RAG（Retrieval-Augmented Generation）系统演示。

## 功能说明

这个 RAG 系统实现了以下功能：

1. **文档索引构建**：使用 Qwen3-Embedding-0.6B 模型为知识库文档生成嵌入向量
2. **向量缓存机制**：自动保存和加载向量化结果，避免重复计算
3. **智能缓存更新**：自动检测知识库变化，仅在需要时重新构建索引
4. **语义检索**：根据用户查询，检索最相关的文档（基于余弦相似度）
5. **答案生成**：使用 Qwen3-VL 模型基于检索到的文档生成答案

## 工作流程

```
用户问题
    ↓
生成查询嵌入向量
    ↓
计算与文档的相似度
    ↓
检索 Top-K 相关文档
    ↓
构建包含上下文的提示词
    ↓
使用 Qwen3-VL 生成答案
    ↓
返回答案
```

## 使用方法

### 1. 确保依赖已安装

```bash
pip install transformers torch
```

### 2. 确保 Ollama 服务运行

```bash
# 检查 Ollama 是否运行
curl http://127.0.0.1:11434/api/tags

# 如果没有运行，启动 Ollama 服务
ollama serve
```

### 3. 确保已下载 Qwen3-VL 模型

```bash
ollama pull qwen3-vl:2b-instruct-q4_K_M
```

### 4. 运行 RAG 演示

```bash
cd embedding
python rag_demo.py
```

**首次运行**：首次运行时会构建索引并保存到缓存（`embedding/cache/` 目录）

**后续运行**：如果知识库没有变化，会自动加载缓存的向量，大幅提升启动速度

**强制重建索引**：如果知识库有更新或想强制重建，使用 `--rebuild` 参数：

```bash
python rag_demo.py --rebuild
```

## 代码结构

### `rag_demo.py` 主要组件

- **`SimpleRAG` 类**：RAG 系统主类
  - `build_index()`: 构建文档索引（自动使用缓存）
  - `save_index()`: 保存索引到缓存文件
  - `load_index()`: 从缓存文件加载索引
  - `is_index_valid()`: 检查缓存索引是否仍然有效
  - `retrieve()`: 检索相关文档
  - `generate_answer()`: 生成答案
  - `query()`: 完整的查询流程

### 关键函数

- **`last_token_pool()`**: 从模型隐藏状态提取嵌入向量
- **`get_detailed_instruct()`**: 为查询添加任务指令
- **`encode_texts()`**: 将文本编码为嵌入向量

## 示例输出

```
用户问题: 虎皮鹦鹉有什么特征？

检索到 2 个相关文档:
  [1] 相似度: 0.856 - # 虎皮鹦鹉...
  [2] 相似度: 0.234 - # 玄凤鹦鹉...

生成的答案:
根据检索到的文档，虎皮鹦鹉的主要特征包括：
1. 体型小，额部平滑无冠羽
2. 颈部有黑色斑点，翅膀有波浪纹
3. 常见绿色/蓝色羽色
...
```

## 配置参数

在 `SimpleRAG` 初始化时可以调整以下参数：

- `embedding_model_name`: 嵌入模型名称（默认: 'Qwen/Qwen3-Embedding-0.6B'）
- `llm_model_name`: 生成模型名称（默认: 'qwen3-vl:2b-instruct-q4_K_M'）
- `llm_temperature`: LLM 温度参数（默认: 0.7）
- `knowledge_base_path`: 知识库目录路径（默认: '../knowledge'）
- `task_description`: 检索任务描述

## 向量缓存机制

系统会自动将向量化的文档保存到 `embedding/cache/` 目录：

- **`document_embeddings.pt`**: 保存文档的嵌入向量（PyTorch 格式）
- **`documents.pkl`**: 保存文档内容列表（Pickle 格式）
- **`metadata.json`**: 保存元数据（包含知识库文件哈希、修改时间等）

### 缓存工作原理

1. **首次运行**：构建索引并保存到缓存
2. **后续运行**：
   - 自动检查缓存是否存在
   - 验证知识库是否有更新（通过文件哈希）
   - 如果缓存有效，直接加载（秒级启动）
   - 如果知识库有更新，自动重新构建索引

### 手动管理缓存

- **清除缓存**：删除 `embedding/cache/` 目录即可
- **强制重建**：使用 `--rebuild` 参数或删除缓存文件

## 注意事项

1. **首次运行**：首次运行时会下载嵌入模型，可能需要一些时间
2. **内存要求**：嵌入模型和 LLM 模型都需要一定的内存空间
3. **知识库路径**：确保 `knowledge_base_path` 指向正确的知识库目录
4. **Ollama 服务**：确保 Ollama 服务正在运行，否则无法调用生成模型
5. **缓存目录**：缓存文件存储在 `embedding/cache/` 目录，可以安全删除以释放空间

## 性能优化

- **向量缓存**：已实现，避免重复向量化
- **批量处理**：文档向量化采用批量处理，提升效率
- **GPU 加速**：如果有 GPU，可以自动使用 GPU 加速向量化

## 扩展建议

1. **向量数据库**：对于大量文档，可以使用 FAISS、Milvus 等向量数据库替代文件缓存
2. **文档分块**：对于长文档，可以分割成多个块分别索引，提升检索精度
3. **查询缓存**：可以缓存常见查询的答案，进一步提升响应速度
4. **多轮对话**：可以添加对话历史支持多轮问答
5. **增量更新**：支持增量添加新文档，无需重建整个索引

